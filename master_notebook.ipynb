{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"master_notebook.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1MCgXypxLSOo","colab_type":"text"},"source":["# <center>AI gamer that will never win with RL</center>\n","\n","---\n","|Name|UID| \n","|---|---|\n","|Chan Tsai Hor|3035376355|\n","|Chan Sik Yuen|3035375129|"]},{"cell_type":"markdown","metadata":{"id":"fK2dtPwo5qfE","colab_type":"text"},"source":["<li>\n","  https://rubenfiszel.github.io/posts/rl4j/2016-08-24-Reinforcement-Learning-and-DQN.html\n","\n","</li>\n"]},{"cell_type":"markdown","metadata":{"id":"58ZRzHTag96j","colab_type":"text"},"source":["# Changelog (Internal Use ONLY)\n","Please use the format of:\n","\n","```\n","YYYY-MM-DD-HHMM (Editor):\n","* Change 1\n","* Change 2\n"," - Change 2.1\n","    + Change 2.1.1\n","```"]},{"cell_type":"markdown","metadata":{"id":"e-9ffkezp8r_","colab_type":"text"},"source":["---\n","__2020-04-14-1947__ (NCSY):\n","* Added a comment of dependency conflict of TF-DQN with gym-connect-four.\n","* Splitted copied code from gym-connect-four into individual code cells.\n","* Modified the block comment (reward for invalid move).\n","* Added the descriptions of Reward Specification."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IvoqgYTQp61I"},"source":["---\n","# <h2>Install dependencies</h2>\n","\n","<li>https://github.com/IASIAI/gym-connect-four</li>"]},{"cell_type":"code","metadata":{"id":"fw9C-ruPWmh9","colab_type":"code","outputId":"ef2da0a0-2103-4eb3-9219-b8219f2c88f0","executionInfo":{"status":"ok","timestamp":1587651789555,"user_tz":-480,"elapsed":4858,"user":{"displayName":"Tsai Hor Chan","photoUrl":"","userId":"00447202594890395073"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["\n","# Install connect-4 environment\n","# !git clone https://github.com/nicolas-chan-42/COMP-3359-Project-Deep-Q-RL.git\n","\n","!git clone https://username:password@github.com/nicolas-chan-42/COMP-3359-Project-Deep-Q-RL.git\n","\n","!pip install -e ./gym-connect-four\n","%cd gym-connect-four"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Cloning into 'COMP-3359-Project-Deep-Q-RL'...\n","remote: Invalid username or password.\n","fatal: Authentication failed for 'https://username:password@github.com/nicolas-chan-42/COMP-3359-Project-Deep-Q-RL.git/'\n","\u001b[31mERROR: ./gym-connect-four is not a valid editable requirement. It should either be a path to a local project or a VCS URL (beginning with svn+, git+, hg+, or bzr+).\u001b[0m\n","[Errno 2] No such file or directory: 'gym-connect-four'\n","/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FCU2tJAe1Roi","colab_type":"text"},"source":["#<h2>Import Libraries</h2>"]},{"cell_type":"code","metadata":{"id":"HmEUkxHvWHGw","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function\n","\n","import base64\n","import imageio\n","import IPython\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import PIL.Image\n","import pyvirtualdisplay\n","import tensorflow as tf\n","\n","from tf_agents.agents.dqn import dqn_agent\n","from tf_agents.drivers import dynamic_step_driver\n","from tf_agents.environments import suite_gym\n","from tf_agents.environments import tf_py_environment\n","from tf_agents.eval import metric_utils\n","from tf_agents.metrics import tf_metrics\n","from tf_agents.networks import q_network\n","from tf_agents.policies import random_tf_policy\n","from tf_agents.replay_buffers import tf_uniform_replay_buffer\n","from tf_agents.trajectories import trajectory\n","from tf_agents.utils import common"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_XQOui6MEVg5","colab_type":"text"},"source":["---\n","# Hyperparameters"]},{"cell_type":"markdown","metadata":{"id":"QYV9RD6u_ufQ","colab_type":"text"},"source":["<font size=\"2\" color=\"Red\">Need to change variable names later to match checkpoints</font>"]},{"cell_type":"code","metadata":{"id":"ZkoTrNQJEauB","colab_type":"code","cellView":"code","colab":{}},"source":["\n","'''---Hyperparameters for Connect4---'''\n","\n","'''For deep Q network'''\n","\n","\n","##### Experience Replay #####\n","# MEM_BUFFER_SIZE: max number of transitions to store in memory buffer\n","# BATCH_SIZE: batch size of transitions to be sampled during Experience Replay,\n","#             i.e. DQN training data input batch size\n","initial_collect_steps = 1000  # @param {type:\"integer\"} \n","collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n","replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n","BATCH_SIZE = 64  # @param {type:\"integer\"}\n","\n","##### DQN Model #####\n","# GAMMA: discount rate of expected cumulative rewards, \n","#        as in Q(s,a) = r + GAMMA * max_a'( Q(s',a') )\n","# LR: learning rate of DQN model\n","# LAMBDA: weight for regularization of network weights (as in \"weight_dacay\" parameter \n","#         in PyTorch's Adam optimizer). Regularization is applied to avoid network weights \n","#         have large values, and try to prevent overfitting of the model.\n","# TARGET_UPDATE_PER_STEPS: Target DQN will be updated every TARGET_UPDATE_PER_STEPS\n","#                          in training phase.\n","\n","LR = 1e-3  # @param {type:\"number\"}\n","log_interval = 200  # @param {type:\"integer\"}\n","\n","##### Episodic Training #####\n","N_EVAL_EPISODES = 10  # @param {type:\"integer\"}\n","num_iterations = 20000 # @param {type:\"integer\"}\n","eval_interval = 1000  # @param {type:\"integer\"}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VGw0tY6OLpd3","colab_type":"text"},"source":["---\n","# <h2>Set up Connect 4 environment</h2>\n","\n","<h3> Tasks </h3>\n","<li> Customize run function </li>\n","<li> Write customized players (random/nn players) </li>\n"]},{"cell_type":"code","metadata":{"id":"hi01rFCBgsOR","colab_type":"code","outputId":"e124b197-f0ff-4ae9-d7a3-77db6e5abeb7","executionInfo":{"status":"ok","timestamp":1586874161837,"user_tz":-480,"elapsed":36784,"user":{"displayName":"Tsai Hor Chan","photoUrl":"","userId":"00447202594890395073"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["import gym\n","import numpy as np\n","\n","from abc import ABC, abstractmethod\n","from gym_connect_four import ConnectFourEnv"],"execution_count":0,"outputs":[{"output_type":"stream","text":["pygame 1.9.6\n","Hello from the pygame community. https://www.pygame.org/contribute.html\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BIHUlC7kgf0_","cellView":"code","colab":{}},"source":["class Player(ABC):\n","    \"\"\" Class used for evaluating the game \"\"\"\n","\n","    def __init__(self, env: 'ConnectFourEnv', name='Player'):\n","        self.name = name\n","        self.env = env\n","\n","    @abstractmethod\n","    def get_next_action(self, state: np.ndarray) -> int:\n","        pass\n","\n","    def learn(self, state, action: int, state_next, reward: int, done: bool) -> None:\n","        pass\n","\n","    def save_model(self, model_prefix: str = None):\n","        raise NotImplementedError()\n","\n","    def load_model(self, model_prefix: str = None):\n","        raise NotImplementedError()\n","\n","    def reset(self, episode: int = 0, side: int = 1) -> None:\n","        \"\"\"\n","        Allows a player class to reset it's state before each round\n","            Parameters\n","            ----------\n","            episode : which episode we have reached\n","            side : 1 if the player is starting or -1 if the player is second\n","        \"\"\"\n","        pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"igGo9gxMg2QD","colab_type":"code","colab":{}},"source":["#Just copy it here for now, can self-write one afterwards\n","class RandomPlayer(Player):\n","    def __init__(self, env: 'ConnectFourEnv', name='RandomPlayer', seed = None):\n","        super().__init__(env, name)\n","        self._seed = seed\n","        # For reproducibility of the random\n","        prev_state = random.getstate()\n","        random.seed(self._seed)\n","        self._state = random.getstate()\n","        random.setstate(prev_state)\n","\n","    def get_next_action(self, state: np.ndarray) -> int:\n","        available_moves = self.env.available_moves()\n","        if not available_moves:\n","            raise ValueError('Unable to determine a valid move! Maybe invoke at the wrong time?')\n","\n","        # Next operations are needed for reproducibility of the RandomPlayer when inited with seed\n","        prev_state = random.getstate()\n","        random.setstate(self._state)\n","        action = random.choice(list(available_moves))\n","        self._state = random.getstate()\n","        random.setstate(prev_state)\n","        return action\n","\n","    def reset(self, episode: int = 0, side: int = 1) -> None:\n","        # For reproducibility of the random\n","        random.seed(self._seed)\n","        self._state = random.getstate()\n","\n","    def save_model(self, model_prefix: str = None):\n","        pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6r4Q53Ogg6T0","colab_type":"code","colab":{}},"source":["#Inherent from original ConnectFourEnv\n","class myConnectFourEnv (ConnectFourEnv):\n","\n","  LOSS_REWARD = 1\n","  DEF_REWARD = 0\n","  DRAW_REWARD = -0.1\n","  WIN_REWARD = -1\n","\n","  #board may change to other shapes for testing?\n","  #Just copied everything, still have little idea how \n","  def run(self, player1, player2, board=None, render=False):\n","    #Reset the running environment\n","    player1.reset()\n","    player2.reset()\n","    self.reset(board)\n","\n","    cp = lambda: self.__current_player\n","\n","    def change_player():\n","        self.__current_player *= -1\n","        return player1 if cp() == 1 else player2\n","\n","    state_hist = deque([self.__board.copy()], maxlen=4)\n","\n","    act = player1.get_next_action(self.__board * 1)\n","    act_hist = deque([act], maxlen=2)\n","    step_result = self._step(act)\n","    state_hist.append(self.__board.copy())\n","    player = change_player()\n","    done = False\n","    while not done:\n","        if render:\n","            self.render()\n","        act_hist.append(player.get_next_action(self.__board * cp()))\n","        step_result = self._step(act_hist[-1])\n","        state_hist.append(self.__board.copy())\n","\n","        player = change_player()\n","\n","        reward = step_result.get_reward(cp())\n","        done = step_result.is_done()\n","        player.learn(state=state_hist[-3] * cp(), action=act_hist[-2], state_next=state_hist[-1] * cp(), reward=reward, done=done)\n","\n","    player = change_player()\n","    reward = step_result.get_reward(cp())\n","    player.learn(state_hist[-2] * cp(), act_hist[-1], state_hist[-1] * cp(), reward, done)\n","    if render:\n","        self.render()\n","\n","    return step_result.res_type"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TcOboVRQzJ1F","colab_type":"text"},"source":["---\n","## <h2><font color=\"Red\">Issue</font></h2>\n","\n","\n","<font color=\"Red\">My Connect4 was not loaded</font>\n","\n","Need to follow the website below for registering our new environment. Can do that later when we have complete our test.\n","https://github.com/openai/gym/wiki/Environments\n","\n","<font color=\"LightGreen\">Thought: Can link this colab notebook with Github</font>"]},{"cell_type":"code","metadata":{"id":"eg0uBcW8Lo8G","colab_type":"code","outputId":"9c7e4a9d-3377-404e-ce73-e537d3d44f9d","executionInfo":{"status":"ok","timestamp":1586874161839,"user_tz":-480,"elapsed":36758,"user":{"displayName":"Tsai Hor Chan","photoUrl":"","userId":"00447202594890395073"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import gym\n","from gym_connect_four import RandomPlayer, ConnectFourEnv\n","env_name = 'ConnectFour-v0'\n","\n","env: ConnectFourEnv = gym.make(env_name)\n","\n","\n","player1 = RandomPlayer(env, 'Dexter-Bot')\n","print(player1)\n","player2 = RandomPlayer(env, 'Deedee-Bot')\n","result = env.run(player1, player2, render=True)\n","reward = result.value\n","print(reward)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<gym_connect_four.envs.connect_four_env.RandomPlayer object at 0x7fa2192b6f60>\n","|---+---+---+---+---+---+---|\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","| A |   |   |   |   |   |   |\n","|---+---+---+---+---+---+---|\n","|---+---+---+---+---+---+---|\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","| A | B |   |   |   |   |   |\n","|---+---+---+---+---+---+---|\n","|---+---+---+---+---+---+---|\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","| A | B |   |   |   |   | A |\n","|---+---+---+---+---+---+---|\n","|---+---+---+---+---+---+---|\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","| A | B |   |   | B |   | A |\n","|---+---+---+---+---+---+---|\n","|---+---+---+---+---+---+---|\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   | A |\n","| A | B |   |   | B |   | A |\n","|---+---+---+---+---+---+---|\n","|---+---+---+---+---+---+---|\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   | A |\n","| A | B |   | B | B |   | A |\n","|---+---+---+---+---+---+---|\n","|---+---+---+---+---+---+---|\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   | A |\n","|   |   |   |   |   |   | A |\n","| A | B |   | B | B |   | A |\n","|---+---+---+---+---+---+---|\n","|---+---+---+---+---+---+---|\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   | A |\n","|   |   |   |   |   |   | A |\n","| A | B |   | B | B | B | A |\n","|---+---+---+---+---+---+---|\n","|---+---+---+---+---+---+---|\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   |   |\n","|   |   |   |   |   |   | A |\n","|   |   |   |   |   |   | A |\n","|   |   |   |   |   |   | A |\n","| A | B |   | B | B | B | A |\n","|---+---+---+---+---+---+---|\n","1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L3sFxlA_1OOO","colab_type":"text"},"source":["---\n","# <h2>Environment Specifications</h2>\n","\n","The `env.step` method takes an `action` in the environment and returns a `StepResult` tuple containing the next observation of the environment and the reward for the action.\n","\n","\n","\n","(Copied from tf tutorial https://www.tensorflow.org/agents/tutorials/)\n","(Modified a bit...)"]},{"cell_type":"code","metadata":{"id":"C1-HbaJsrG_m","colab_type":"code","outputId":"aeb5b7b5-7990-436c-ff04-2db11556321b","executionInfo":{"status":"ok","timestamp":1586874161839,"user_tz":-480,"elapsed":36749,"user":{"displayName":"Tsai Hor Chan","photoUrl":"","userId":"00447202594890395073"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["\"\"\"\n","Description:\n","    ConnectFour game environment\n","Observation:\n","    Type: Discreet(6,7)\n","Actions:\n","    Type: Discreet(7)\n","    Num     Action\n","    x       Column in which to insert next token (0-6)\n","Reward:\n","    Reward is 0 for every step.\n","    If there are no other further steps possible, Reward is -0.5 and termination will occur\n","    If it's a win condition, Reward will be -1 and termination will occur\n","    If it is an invalid move, Reward will be -1 and termination will occur\n","Starting State:\n","    All observations are assigned a value of 0\n","Episode Termination:\n","    No more spaces left for pieces\n","    4 pieces are present in a line: horizontal, vertical or diagonally\n","    An attempt is made to place a piece in an invalid location\n","\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nDescription:\\n    ConnectFour game environment\\nObservation:\\n    Type: Discreet(6,7)\\nActions:\\n    Type: Discreet(7)\\n    Num     Action\\n    x       Column in which to insert next token (0-6)\\nReward:\\n    Reward is 0 for every step.\\n    If there are no other further steps possible, Reward is -0.5 and termination will occur\\n    If it's a win condition, Reward will be -1 and termination will occur\\n    If it is an invalid move, Reward will be -1 and termination will occur\\nStarting State:\\n    All observations are assigned a value of 0\\nEpisode Termination:\\n    No more spaces left for pieces\\n    4 pieces are present in a line: horizontal, vertical or diagonally\\n    An attempt is made to place a piece in an invalid location\\n\""]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"_Jak_LpY1VBN","colab_type":"code","outputId":"2b62cfd5-fb48-4d33-f022-5e17c121a89d","executionInfo":{"status":"ok","timestamp":1586876501134,"user_tz":-480,"elapsed":832,"user":{"displayName":"Tsai Hor Chan","photoUrl":"","userId":"00447202594890395073"}},"colab":{"base_uri":"https://localhost:8080/","height":377}},"source":["\n","time_step = env.reset()\n","print('Time step:')\n","print(time_step)\n","\n","action = np.array(1, dtype=np.int32)\n","\n","next_time_step = env.step(action)\n","print('Next time step:')\n","print(next_time_step)\n","\n","#Observation Specification\n","print(\"---Observation Specification\")\n","print(env.observation_space)\n","\n","\n","#Reward specification\n","print(\"---Reward Specification\")\n","#???\n","### In class ConnectFourEnv:\n","#   LOSS_REWARD = -1\n","#   DEF_REWARD = 0\n","#   DRAW_REWARD = 0.5\n","#   WIN_REWARD = 1\n","### Reward is get from StepResult.get_reward(player: int).\n","\n","#Action Specification\n","print(\"---Action Specification\")\n","print(env.action_space)\n","\n","\n","#Basically a section telling people what the environment is \n","#How do we define action, reward or observation\n","\n","#More investigation needed on the reward specifications\n","\n","'''Set up training environments'''\n","\n","#Convert to Tensorflow format\n","train_py_env = suite_gym.load(env_name)\n","eval_py_env = suite_gym.load(env_name)\n","\n","train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n","eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Time step:\n","[[0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0]]\n","Next time step:\n","(array([[0, 0, 0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0, 0, 0],\n","       [0, 1, 0, 0, 0, 0, 0]]), 0, False, {})\n","---Observation Specification\n","Box(6, 7)\n","---Reward Specification\n","---Action Specification\n","Discrete(7)\n","TimeStep(step_type=ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'), reward=ArraySpec(shape=(), dtype=dtype('float32'), name='reward'), discount=BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0), observation=BoundedArraySpec(shape=(6, 7), dtype=dtype('int64'), name='observation', minimum=-1, maximum=1))\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DEixd5bUxbZp","colab_type":"text"},"source":["---\n","# <h2>Deep Q network Construction</h2>\n","\n","The DQN agent can be used in any environment which has a discrete action space.\n","\n","At the heart of a DQN Agent is a `QNetwork`, a neural network model that can learn to predict `QValues` (expected returns) for all actions, given an observation from the environment.\n","\n","Use `tf_agents.networks.q_network` to create a `QNetwork`, passing in the `observation_spec`, `action_spec`, and a tuple describing the number and size of the model's hidden layers.\n","\n","(Copied from tf tutorial https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial)\n","\n","<h3>Issues</h3>\n","\n","\n","\n","*   Optimizers can change to our own? (Like from the checkpoint)\n","*   Agent just copied from predefined, can write one later using the example in checkpoint\n","\n"]},{"cell_type":"code","metadata":{"id":"VfwQ0nEEI89j","colab_type":"code","colab":{}},"source":["\"\"\" Deep Q Network by tensorflow\"\"\"\n","\n","fc_layer_params = (100,)\n","\n","q_net = q_network.QNetwork(\n","    train_env.observation_spec(),\n","    train_env.action_spec(),\n","    fc_layer_params=fc_layer_params)\n","\n","#Set up optimizer \n","\n","optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=LR)\n","\n","#DQN Agent\n","train_step_counter = tf.Variable(0)\n","\n","agent = dqn_agent.DqnAgent(\n","    train_env.time_step_spec(),\n","    train_env.action_spec(),\n","    q_network=q_net,\n","    optimizer=optimizer,\n","    td_errors_loss_fn=common.element_wise_squared_loss,\n","    train_step_counter=train_step_counter)\n","\n","agent.initialize()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"66UBNXofEngD","colab_type":"text"},"source":["---\n","# <h2>Policies</h2>\n","\n","A policy defines the way an agent acts in an environment. Typically, the goal of reinforcement learning is to train the underlying model until the policy produces the desired outcome.\n","\n","*  `agent.policy` — The main policy that is used for evaluation and deployment.\n","\n","*  `agent.collect_policy` — A second policy that is used for data collection.\n","\n","\n","To get an action from a policy, call the `policy.action(time_step)` method. The `time_step` contains the observation from the environment. This method returns a `PolicyStep`, which is a named tuple with three components:\n","\n","*  `action` — the action to be taken (in this case, `0` or `1`)\n","*  `state` — used for stateful (that is, RNN-based) policies\n","*  `info` — auxiliary data, such as log probabilities of actions\n","\n","(Copied from tf tutorial https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial)\n"]},{"cell_type":"code","metadata":{"id":"SAX2EW08GBmF","colab_type":"code","colab":{}},"source":["'''\n","Can customize policies as followed\n","\n","Policies can be created independently of agents. For example, use tf_agents.policies.random_tf_policy \n","to create a policy which will randomly select an action for each time_step.\n","\n","random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n","                                                train_env.action_spec())\n","'''\n","\n","eval_policy = agent.policy\n","collect_policy = agent.collect_policy"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fSbWpsDyHr2p","colab_type":"text"},"source":["---\n","# <h2>Metrics and Evaluation</h2>\n","\n","The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n","\n","(Copied from https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial)\n","\n","<font size=\"2\" color=\"Red\">We may just use rewards instead of averaged rewards</font>"]},{"cell_type":"code","metadata":{"id":"WlG6MbY7HraD","colab_type":"code","colab":{}},"source":["def compute_avg_return(environment, policy, num_episodes=10):\n","\n","  total_return = 0.0\n","  for _ in range(num_episodes):\n","\n","    time_step = environment.reset()\n","    episode_return = 0.0\n","\n","    while not time_step.is_last():\n","      action_step = policy.action(time_step)\n","      time_step = environment.step(action_step.action)\n","      episode_return += time_step.reward\n","    total_return += episode_return\n","\n","  avg_return = total_return / num_episodes\n","  return avg_return.numpy()[0]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x0A_7h-BZtQh","colab_type":"text"},"source":["---\n","# <h2>Replay Buffer</h2>\n","\n","The replay buffer keeps track of data collected from the environment. This tutorial uses `tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer`, as it is the most common. \n","\n","The constructor requires the specs for the data it will be collecting. This is available from the agent using the `collect_data_spec` method. The batch size and maximum buffer length are also required.\n","\n","For most agents, collect_data_spec is a named tuple called Trajectory, containing the specs for observations, actions, rewards, and other items.\n","\n","(copied from https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial)\n"]},{"cell_type":"code","metadata":{"id":"OtbDqiHEaCG7","colab_type":"code","outputId":"30ee0733-85a9-4179-c830-5ca86367eebb","executionInfo":{"status":"ok","timestamp":1586874162463,"user_tz":-480,"elapsed":37322,"user":{"displayName":"Tsai Hor Chan","photoUrl":"","userId":"00447202594890395073"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n","    data_spec=agent.collect_data_spec,\n","    batch_size=train_env.batch_size,\n","    max_length=replay_buffer_max_length)\n","\n","agent.collect_data_spec"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Trajectory(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), observation=BoundedTensorSpec(shape=(6, 7), dtype=tf.int64, name='observation', minimum=array(-1), maximum=array(1)), action=BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(6)), policy_info=(), next_step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)))"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"orynP6teaTfl","colab_type":"text"},"source":["---\n","# <h2>Data Collection</h2>\n","\n","(copied from https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial)\n"]},{"cell_type":"code","metadata":{"id":"__pH5zeVaioT","colab_type":"code","colab":{}},"source":["def collect_step(environment, policy, buffer):\n","  time_step = environment.current_time_step()\n","  action_step = policy.action(time_step)\n","  next_time_step = environment.step(action_step.action)\n","  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n","\n","  # Add trajectory to the replay buffer\n","  buffer.add_batch(traj)\n","\n","def collect_data(env, policy, buffer, steps):\n","  for _ in range(steps):\n","    collect_step(env, policy, buffer)\n","\n","\n","\n","# This loop is so common in RL, that we provide standard implementations. \n","# For more details see the drivers module.\n","# https://github.com/tensorflow/agents/blob/master/tf_agents/docs/python/tf_agents/drivers.md\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_5qhXT5Maul2","colab_type":"text"},"source":["The agent needs access to the replay buffer. This is provided by creating an iterable `tf.data.Dataset` pipeline which will feed data to the agent.\n","\n","Each row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (`num_steps=2`).\n","\n","This dataset is also optimized by running parallel calls and prefetching data."]},{"cell_type":"code","metadata":{"id":"RsFDeeHRa1Qz","colab_type":"code","outputId":"588d7dd8-f507-4b68-f4fe-d874efcbd349","executionInfo":{"status":"ok","timestamp":1586874428489,"user_tz":-480,"elapsed":946,"user":{"displayName":"Tsai Hor Chan","photoUrl":"","userId":"00447202594890395073"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# Dataset generates trajectories with shape [Bx2x...]\n","dataset = replay_buffer.as_dataset(\n","    num_parallel_calls=3, \n","    sample_batch_size=BATCH_SIZE, \n","    num_steps=2).prefetch(3)\n","\n","print(dataset)\n","\n","iterator = iter(dataset)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<PrefetchDataset shapes: (Trajectory(step_type=(64, 2), observation=(64, 2, 6, 7), action=(64, 2), policy_info=(), next_step_type=(64, 2), reward=(64, 2), discount=(64, 2)), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(step_type=tf.int32, observation=tf.int64, action=tf.int64, policy_info=(), next_step_type=tf.int32, reward=tf.float32, discount=tf.float32), BufferInfo(ids=tf.int64, probabilities=tf.float32))>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2qkMU0dojf2_","colab_type":"text"},"source":["---\n","# <h2>Main Training Loop</h2>\n","\n","Two things must happen during the training loop:\n","*   collect data from the environment\n","*   use that data to train the agent's neural network(s)\n","\n","This example also periodicially evaluates the policy and prints the current score.\n","\n","\n","The following will take ~5 minutes to run."]},{"cell_type":"code","metadata":{"id":"reJ0hi9kkzCL","colab_type":"code","outputId":"834fb1e9-92ea-4960-fcc7-196799df7ec5","executionInfo":{"status":"ok","timestamp":1586874737680,"user_tz":-480,"elapsed":170604,"user":{"displayName":"Tsai Hor Chan","photoUrl":"","userId":"00447202594890395073"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["try:\n","  %%time\n","except:\n","  pass\n","\n","# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n","agent.train = common.function(agent.train)\n","\n","# Reset the train step\n","agent.train_step_counter.assign(0)\n","\n","# Evaluate the agent's policy once before training.\n","avg_return = compute_avg_return(eval_env, agent.policy, N_EVAL_EPISODES)\n","returns = [avg_return]\n","\n","for _ in range(num_iterations):\n","\n","  # Collect a few steps using collect_policy and save to the replay buffer.\n","  for _ in range(collect_steps_per_iteration):\n","    collect_step(train_env, agent.collect_policy, replay_buffer)\n","\n","  # Sample a batch of data from the buffer and update the agent's network.\n","  experience, unused_info = next(iterator)\n","  train_loss = agent.train(experience).loss\n","\n","  step = agent.train_step_counter.numpy()\n","\n","  if step % log_interval == 0:\n","    print('step = {0}: loss = {1}'.format(step, train_loss))\n","\n","  if step % eval_interval == 0:\n","    avg_return = compute_avg_return(eval_env, agent.policy, N_EVAL_EPISODES)\n","    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n","    returns.append(avg_return)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n","Wall time: 5.01 µs\n","step = 200: loss = 1.514430284500122\n","step = 400: loss = 0.12062457203865051\n","step = 600: loss = 0.031146036460995674\n","step = 800: loss = 0.031332362443208694\n","step = 1000: loss = 0.011314150877296925\n","step = 1000: Average Return = 1.0\n","step = 1200: loss = 0.004268316552042961\n","step = 1400: loss = 0.005487052258104086\n","step = 1600: loss = 0.0030439468100667\n","step = 1800: loss = 0.004347688518464565\n","step = 2000: loss = 0.002133280271664262\n","step = 2000: Average Return = 1.0\n","step = 2200: loss = 0.0018653888255357742\n","step = 2400: loss = 0.0015920099103823304\n","step = 2600: loss = 0.0009811773197725415\n","step = 2800: loss = 0.0006876069819554687\n","step = 3000: loss = 0.0015183452051132917\n","step = 3000: Average Return = 1.0\n","step = 3200: loss = 0.0009164385846816003\n","step = 3400: loss = 0.0009640451753512025\n","step = 3600: loss = 0.0008763040532357991\n","step = 3800: loss = 0.0008140333229675889\n","step = 4000: loss = 0.0009360619005747139\n","step = 4000: Average Return = 1.0\n","step = 4200: loss = 0.0005028424202464521\n","step = 4400: loss = 0.0006078429287299514\n","step = 4600: loss = 0.0006275812047533691\n","step = 4800: loss = 0.0012327288277447224\n","step = 5000: loss = 0.001499361707828939\n","step = 5000: Average Return = 1.0\n","step = 5200: loss = 0.0010827265214174986\n","step = 5400: loss = 0.0006634733872488141\n","step = 5600: loss = 0.0010419595055282116\n","step = 5800: loss = 0.004932372365146875\n","step = 6000: loss = 0.015659447759389877\n","step = 6000: Average Return = 1.0\n","step = 6200: loss = 0.01387824583798647\n","step = 6400: loss = 0.0025507519021630287\n","step = 6600: loss = 0.0008160104043781757\n","step = 6800: loss = 0.0006216384936124086\n","step = 7000: loss = 0.00047440064372494817\n","step = 7000: Average Return = 1.0\n","step = 7200: loss = 0.0006750753382220864\n","step = 7400: loss = 0.0005157641135156155\n","step = 7600: loss = 0.0005283394129946828\n","step = 7800: loss = 0.0005567156476899981\n","step = 8000: loss = 0.00032545760041102767\n","step = 8000: Average Return = 1.0\n","step = 8200: loss = 0.0002958925615530461\n","step = 8400: loss = 0.0002995042013935745\n","step = 8600: loss = 0.0003192198055330664\n","step = 8800: loss = 0.0005575163522735238\n","step = 9000: loss = 0.0010275806998834014\n","step = 9000: Average Return = 1.0\n","step = 9200: loss = 0.0010759367141872644\n","step = 9400: loss = 0.00025765132158994675\n","step = 9600: loss = 0.0002031060284934938\n","step = 9800: loss = 0.00029409589478746057\n","step = 10000: loss = 0.00024002573627512902\n","step = 10000: Average Return = 1.0\n","step = 10200: loss = 0.00014693289995193481\n","step = 10400: loss = 0.0002702048805076629\n","step = 10600: loss = 0.00015777969383634627\n","step = 10800: loss = 0.00019845641509164125\n","step = 11000: loss = 0.004259985871613026\n","step = 11000: Average Return = 1.0\n","step = 11200: loss = 0.000949978712014854\n","step = 11400: loss = 0.00036361924139782786\n","step = 11600: loss = 0.00022071463172324002\n","step = 11800: loss = 0.00021536368876695633\n","step = 12000: loss = 0.00021245934476610273\n","step = 12000: Average Return = 1.0\n","step = 12200: loss = 0.00023785144730936736\n","step = 12400: loss = 0.00020260989549569786\n","step = 12600: loss = 0.00015103930491022766\n","step = 12800: loss = 0.0001346631470369175\n","step = 13000: loss = 0.00010026402014773339\n","step = 13000: Average Return = 1.0\n","step = 13200: loss = 0.0001465044915676117\n","step = 13400: loss = 0.00011868966976180673\n","step = 13600: loss = 8.846627315506339e-05\n","step = 13800: loss = 0.00011788099072873592\n","step = 14000: loss = 9.900453733280301e-05\n","step = 14000: Average Return = 1.0\n","step = 14200: loss = 0.00012723921099677682\n","step = 14400: loss = 0.0001561700482852757\n","step = 14600: loss = 0.00028944481164216995\n","step = 14800: loss = 0.0007874323055148125\n","step = 15000: loss = 0.00032958961674012244\n","step = 15000: Average Return = 1.0\n","step = 15200: loss = 0.0003680711961351335\n","step = 15400: loss = 0.0006417793920263648\n","step = 15600: loss = 0.00015223686932586133\n","step = 15800: loss = 0.00015736492059659213\n","step = 16000: loss = 0.00011568058107513934\n","step = 16000: Average Return = 1.0\n","step = 16200: loss = 5.6518845667596906e-05\n","step = 16400: loss = 0.00017722719348967075\n","step = 16600: loss = 0.00011148917837999761\n","step = 16800: loss = 0.00011922611884074286\n","step = 17000: loss = 8.001388050615788e-05\n","step = 17000: Average Return = 1.0\n","step = 17200: loss = 0.001442120410501957\n","step = 17400: loss = 0.0013659740798175335\n","step = 17600: loss = 0.00014544287114404142\n","step = 17800: loss = 0.00010165333515033126\n","step = 18000: loss = 7.595917850267142e-05\n","step = 18000: Average Return = 1.0\n","step = 18200: loss = 6.216186739038676e-05\n","step = 18400: loss = 4.5254833821672946e-05\n","step = 18600: loss = 4.7351106331916526e-05\n","step = 18800: loss = 0.0003132726706098765\n","step = 19000: loss = 0.0024002734571695328\n","step = 19000: Average Return = 1.0\n","step = 19200: loss = 0.00027872860664501786\n","step = 19400: loss = 0.00011357699986547232\n","step = 19600: loss = 5.921493357163854e-05\n","step = 19800: loss = 0.00010433905845275149\n","step = 20000: loss = 5.025150312576443e-05\n","step = 20000: Average Return = 1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1bU_dFcfmXlv","colab_type":"text"},"source":["---\n","# <h2>Visualize training results</h2>\n","\n","Use `matplotlib.pyplot` to chart how the policy improved during training.\n","\n","One iteration of `ConnectFour-v0` consists of 200 time steps. The environment gives a reward of +1 for each step the pole stays up, so the maximum return for one episode is 200. The charts shows the return increasing towards that maximum each time it is evaluated during training. (It may be a little unstable and not increase monotonically each time.)\n","\n","(Copied from https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial, to be modified)"]},{"cell_type":"code","metadata":{"id":"i0EamapPmyVf","colab_type":"code","outputId":"a2331ea3-6904-46f2-a61e-faf9afe6531e","executionInfo":{"status":"ok","timestamp":1586874753936,"user_tz":-480,"elapsed":694,"user":{"displayName":"Tsai Hor Chan","photoUrl":"","userId":"00447202594890395073"}},"colab":{"base_uri":"https://localhost:8080/","height":296}},"source":["import matplotlib.pyplot\n","\n","iterations = range(0, num_iterations + 1, eval_interval)\n","plt.plot(iterations, returns)\n","plt.ylabel('Average Return')\n","plt.xlabel('Iterations')\n","#plt.ylim(top=25)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 0, 'Iterations')"]},"metadata":{"tags":[]},"execution_count":20},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"P0ncKvKO4gA2","colab_type":"text"},"source":["# <h2>(Test case for deep Q network)</h2>"]},{"cell_type":"code","metadata":{"id":"rtzeiRP_4ZsR","colab_type":"code","colab":{}},"source":["\"\"\" DQN Test Case \"\"\"\n","\"\"\"\n","##### Construct Temporary DQN Model for Testing #####\n","# Input dim\n","policy_temp = DQN(in_dim=PARAMS[\"N_STATES\"], out_dim=PARAMS[\"N_ACTIONS\"])\n","\n","##### Test Case #####\n","# Test input state with one state: [[0.33,0.59]]\n","x_temp = torch.tensor([[0.33,0.59]])   # x_temp shape: (batch_size, num_state)\n","\n","# Compute test model output\n","y_temp = policy_temp(x_temp)  # y_temp shape: (batch_size, num_actions)\n","print(\"Test results:\")\n","print(\"y_temp shape: {} (expected: (1,3))\".format(y_temp.shape))\n","print(\"y_temp values: {} (can be any)\".format(y_temp.tolist()))\n","print(\"-----\")\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JywH8KjNyCAU","colab_type":"code","colab":{}},"source":["\n","\"\"\"\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import autograd\n","# Class of DQN model\n","class DQN(nn.Module) :\n","    # Define model structure\n","    def __init__(self, in_dim, out_dim) :\n","\"\"\"\n","        \"\"\"\n","        Inputs:\n","        - in_dim: dimension of input of DQN model, expected to be the \n","                       number of states (N_STATES).\n","        - out_dim: dimension of output of DQN model,\n","                        expected to be the number of possible actions (N_ACTIONS). \n","        (so outputs of DQN model will be predicted Q-Values of input state s for \n","         ALL ACTIONS, i.e. for each input\n","                         state s, returns a vector of Q(s,a) for all actions a)\n","                        \n","        \"\"\"\n","\"\"\"\n","        super(DQN, self).__init__()\n","        \n","        #Use cnn to train\n","        self.in_dim = in_dim\n","        self.out_dim = out_dim\n","        \n","        \n","        self.conv = nn.Sequential(\n","            nn.Linear(self.in_dim, 50),\n","            nn.ReLU(),\n","            nn.Linear(50, 50),\n","            nn.ReLU(),\n","            nn.Linear(50, self.out_dim)\n","        )\n","\n","        #Can replace linear with other neural network\n","    \n","    # (Forward Propagation) Define how input data (x) goes through the DQN model \n","    def forward(self, x):\n","\"\"\"\n","        \"\"\"\n","        Input(s):\n","        - x: Batch of input states, \n","             with shape (batch_size, n_states)\n","\n","        Output(s):\n","        - Predicted Q-Values of current state for all actions, \n","          with shape (batch_size, n_actions)\n","        \"\"\"\n","\"\"\"\n","        # To be completed ...     \n","        x = self.conv(x)\n","        return x\n","\"\"\""],"execution_count":0,"outputs":[]}]}